{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-MongoDB with QLoRA\n",
    "\n",
    "This notebook demonstrates a fine-tuned Qwen2.5-Coder-7B model that translates natural language questions into MongoDB `find` and `aggregate` queries.\n",
    "\n",
    "The base model achieves ~40% accuracy zero-shot. After fine-tuning on ~1,300 synthetic examples using QLoRA (4-bit quantization + LoRA adapters), accuracy jumps to **98.9% on collection schemas it never saw during training**.\n",
    "\n",
    "- Dataset: [jmorenas/text-to-mongo-dataset-qlora](https://huggingface.co/datasets/jmorenas/text-to-mongo-dataset-qlora)\n",
    "- Adapter: [jmorenas/text-to-mongo-qlora](https://huggingface.co/jmorenas/text-to-mongo-qlora)\n",
    "\n",
    "> **Requirements**: A GPU runtime (T4 or better). In Colab: Runtime → Change runtime type → T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers peft bitsandbytes accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset\n",
    "\n",
    "The training data is fully synthetic — no human labeling. Each example contains:\n",
    "- **schema**: Collection name, domain, and fields with typed semantic roles\n",
    "- **allowed_ops**: Which MongoDB operators the model is allowed to use\n",
    "- **intent**: Natural language question\n",
    "- **output**: Ground-truth MongoDB query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO = \"jmorenas/text-to-mongo-dataset-qlora\"\n",
    "\n",
    "def load_jsonl(split: str) -> list[dict]:\n",
    "    \"\"\"Load a JSONL split from HuggingFace, parsing each line as raw JSON.\"\"\"\n",
    "    path = hf_hub_download(repo_id=REPO, filename=f\"{split}.jsonl\", repo_type=\"dataset\")\n",
    "    examples = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "ds = {\n",
    "    \"train\": load_jsonl(\"train\"),\n",
    "    \"eval\": load_jsonl(\"eval\"),\n",
    "    \"held_out\": load_jsonl(\"held_out\"),\n",
    "}\n",
    "\n",
    "for split, examples in ds.items():\n",
    "    print(f\"  {split}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a training example\n",
    "example = ds[\"train\"][0]\n",
    "print(\"Schema:\", json.dumps(example[\"schema\"], indent=2))\n",
    "print(\"\\nAllowed ops:\", json.dumps(example[\"allowed_ops\"], indent=2))\n",
    "print(\"\\nIntent:\", example[\"intent\"])\n",
    "print(\"\\nExpected output:\", json.dumps(example[\"output\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of query types\n",
    "from collections import Counter\n",
    "\n",
    "for split in [\"train\", \"eval\", \"held_out\"]:\n",
    "    types = Counter(ex[\"output\"].get(\"type\", \"unknown\") for ex in ds[split])\n",
    "    negatives = sum(1 for ex in ds[split] if ex[\"is_negative\"])\n",
    "    print(f\"{split}: {dict(types)}, negatives: {negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Model\n",
    "\n",
    "We load the base model (Qwen2.5-Coder-7B-Instruct) in 4-bit precision, then apply the LoRA adapter on top.\n",
    "\n",
    "**Important**: The adapter cannot be merged into 4-bit weights — `merge_and_unload()` silently produces garbage. We keep it as a `PeftModel` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "ADAPTER = \"jmorenas/text-to-mongo-qlora\"\n",
    "\n",
    "# 4-bit quantization config (same as training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {next(model.parameters()).device}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Prompts\n",
    "\n",
    "The model uses ChatML format. The system message instructs it to generate MongoDB queries. The user message contains the schema, allowed operators, and natural language intent.\n",
    "\n",
    "> **Important**: Field descriptions must be short (2-5 words). The model was trained on concise descriptions like `\"Order total\"` or `\"User email\"`. Longer descriptions (e.g. `\"The total monetary value of the order including tax and shipping\"`) cause the model to hallucinate operator lists instead of generating queries. Keep descriptions brief — the semantic role (`measure`, `enum`, `timestamp`, etc.) already carries most of the meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a MongoDB query generator. Given a collection schema, a list of \"\n",
    "    \"allowed operators, and a natural language intent, produce a valid MongoDB \"\n",
    "    \"query as JSON. The output must be a JSON object with a 'type' field \"\n",
    "    \"('aggregate' or 'find') and the corresponding query body. \"\n",
    "    \"If the intent references fields not in the schema, respond with \"\n",
    "    '{\"error\": \"<reason>\"}. '\n",
    "    \"Use only the allowed operators.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_prompt(schema: dict, allowed_ops: dict, intent: str) -> str:\n",
    "    \"\"\"Build a ChatML prompt from schema, allowed ops, and intent.\"\"\"\n",
    "    # Render schema\n",
    "    lines = [f\"Collection: {schema['collection']}\"]\n",
    "    lines.append(\"Fields:\")\n",
    "    for f in schema[\"fields\"]:\n",
    "        parts = [f\"  - {f['name']} ({f['type']}, {f['role']})\"]\n",
    "        if f.get(\"description\"):\n",
    "            parts.append(f\": {f['description']}\")\n",
    "        if f.get(\"enum_values\"):\n",
    "            parts.append(f\" [values: {', '.join(f['enum_values'])}]\")\n",
    "        lines.append(\"\".join(parts))\n",
    "\n",
    "    # Render operators\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Allowed stage operators: \" + \", \".join(allowed_ops[\"stage_operators\"]))\n",
    "    lines.append(\"Allowed expression operators: \" + \", \".join(allowed_ops[\"expression_operators\"]))\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Intent: {intent}\")\n",
    "\n",
    "    user_msg = \"\\n\".join(lines)\n",
    "\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Show what a prompt looks like\n",
    "sample = ds[\"eval\"][0]\n",
    "prompt = build_prompt(sample[\"schema\"], sample[\"allowed_ops\"], sample[\"intent\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference\n",
    "\n",
    "Let's generate queries from examples in the dataset and compare against the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(schema: dict, allowed_ops: dict, intent: str,\n",
    "                   target_model=None, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate a MongoDB query from natural language.\"\"\"\n",
    "    m = target_model or model\n",
    "    prompt = build_prompt(schema, allowed_ops, intent)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(m.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = m.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated tokens (skip the prompt)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few eval examples\n",
    "for i in range(5):\n",
    "    ex = ds[\"eval\"][i]\n",
    "    output = generate_query(ex[\"schema\"], ex[\"allowed_ops\"], ex[\"intent\"])\n",
    "\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Collection: {ex['schema']['collection']}\")\n",
    "    print(f\"Intent: {ex['intent']}\")\n",
    "    print(f\"Expected: {json.dumps(ex['output'])}\")\n",
    "    print(f\"Got:      {output}\")\n",
    "\n",
    "    try:\n",
    "        match = json.loads(output) == ex[\"output\"]\n",
    "        print(f\"Match: {'yes' if match else 'no'}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Match: no (invalid JSON)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generalization — Unseen Schemas\n",
    "\n",
    "The real test: can the model generate correct queries for collection schemas it **never saw during training**?\n",
    "\n",
    "The held-out set contains 3 collections (`museum_exhibits`, `weather_stations`, `fleet_vehicles`) that were excluded from training entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on held-out examples (unseen schemas)\n",
    "held_out_collections = set()\n",
    "for i in range(5):\n",
    "    ex = ds[\"held_out\"][i]\n",
    "    held_out_collections.add(ex[\"schema\"][\"collection\"])\n",
    "    output = generate_query(ex[\"schema\"], ex[\"allowed_ops\"], ex[\"intent\"])\n",
    "\n",
    "    print(f\"--- Held-out {i+1} ---\")\n",
    "    print(f\"Collection: {ex['schema']['collection']} (NEVER seen in training)\")\n",
    "    print(f\"Intent: {ex['intent']}\")\n",
    "    print(f\"Expected: {json.dumps(ex['output'])}\")\n",
    "    print(f\"Got:      {output}\")\n",
    "\n",
    "    try:\n",
    "        match = json.loads(output) == ex[\"output\"]\n",
    "        print(f\"Match: {'yes' if match else 'no'}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Match: no (invalid JSON)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Held-out collections tested: {held_out_collections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try Your Own Schemas\n",
    "\n",
    "The model has never seen these schemas — it reads the field names, types, and roles at inference time to compose queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom schema the model has never seen\n",
    "custom_schema = {\n",
    "    \"collection\": \"books\",\n",
    "    \"domain\": \"library\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"isbn\", \"type\": \"string\", \"role\": \"identifier\", \"description\": \"Book ISBN\"},\n",
    "        {\"name\": \"title\", \"type\": \"string\", \"role\": \"text\", \"description\": \"Book title\"},\n",
    "        {\"name\": \"genre\", \"type\": \"string\", \"role\": \"enum\", \"description\": \"Book genre\",\n",
    "         \"enum_values\": [\"fiction\", \"non-fiction\", \"science\", \"history\", \"biography\"]},\n",
    "        {\"name\": \"pages\", \"type\": \"int\", \"role\": \"measure\", \"description\": \"Page count\"},\n",
    "        {\"name\": \"rating\", \"type\": \"double\", \"role\": \"measure\", \"description\": \"Average rating\"},\n",
    "        {\"name\": \"published_at\", \"type\": \"date\", \"role\": \"timestamp\", \"description\": \"Publication date\"},\n",
    "        {\"name\": \"available\", \"type\": \"bool\", \"role\": \"boolean\", \"description\": \"In stock\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "custom_ops = {\n",
    "    \"stage_operators\": [\"$match\", \"$group\", \"$sort\", \"$limit\", \"$project\"],\n",
    "    \"expression_operators\": [\"$sum\", \"$avg\", \"$gt\", \"$gte\", \"$lt\", \"$eq\", \"$in\"],\n",
    "}\n",
    "\n",
    "queries = [\n",
    "    \"Find all available science books\",\n",
    "    \"What is the average rating per genre?\",\n",
    "    \"Show the top 5 highest-rated fiction books\",\n",
    "    \"How many books are in each genre?\",\n",
    "    \"Find books published after January 2024 with more than 300 pages\",\n",
    "]\n",
    "\n",
    "for intent in queries:\n",
    "    output = generate_query(custom_schema, custom_ops, intent)\n",
    "    print(f\"Q: {intent}\")\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        print(f\"A: {json.dumps(parsed, indent=2)}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"A (raw): {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI/CD Dashboard — Real-world use case\n",
    "\n",
    "This is the schema from a real CI/CD dashboard that tracks container artifacts, product drops, and build pipelines. The model was never trained on any of these collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI/CD Dashboard schemas — artifacts, drops, products\n",
    "artifacts_schema = {\n",
    "    \"collection\": \"artifacts\",\n",
    "    \"domain\": \"cicd_dashboard\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"key\", \"type\": \"string\", \"role\": \"identifier\", \"description\": \"Artifact ID\"},\n",
    "        {\"name\": \"type\", \"type\": \"string\", \"role\": \"enum\", \"description\": \"Artifact type\",\n",
    "         \"enum_values\": [\"containers\", \"disk-images\", \"cloud-disk-images\",\n",
    "                         \"disk-image-containers\", \"cloud-containers\",\n",
    "                         \"base-images\", \"wheels-collections\", \"instructlab\",\n",
    "                         \"models\", \"model-cars\"]},\n",
    "        {\"name\": \"product_key\", \"type\": \"string\", \"role\": \"enum\", \"description\": \"Product key\",\n",
    "         \"enum_values\": [\"rhel-ai\", \"rhaiis\", \"base-images\", \"builder-images\"]},\n",
    "        {\"name\": \"variant\", \"type\": \"string\", \"role\": \"category\", \"description\": \"Accelerator+OS combo\"},\n",
    "        {\"name\": \"archs\", \"type\": \"array\", \"role\": \"category\", \"description\": \"CPU architectures\",\n",
    "         \"enum_values\": [\"x86_64\", \"aarch64\", \"s390x\", \"ppc64le\"]},\n",
    "        {\"name\": \"created_at\", \"type\": \"date\", \"role\": \"timestamp\", \"description\": \"Build timestamp\"},\n",
    "        {\"name\": \"environments\", \"type\": \"array\", \"role\": \"enum\", \"description\": \"Deploy environments\",\n",
    "         \"enum_values\": [\"stage\", \"production\"]},\n",
    "        {\"name\": \"series\", \"type\": \"string\", \"role\": \"category\", \"description\": \"Version series\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "drops_schema = {\n",
    "    \"collection\": \"drops\",\n",
    "    \"domain\": \"cicd_dashboard\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"key\", \"type\": \"string\", \"role\": \"identifier\", \"description\": \"Drop key\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\", \"role\": \"text\", \"description\": \"Drop version name\"},\n",
    "        {\"name\": \"product_key\", \"type\": \"string\", \"role\": \"enum\", \"description\": \"Product key\",\n",
    "         \"enum_values\": [\"rhel-ai\", \"rhaiis\", \"base-images\", \"builder-images\"]},\n",
    "        {\"name\": \"product_version\", \"type\": \"string\", \"role\": \"text\", \"description\": \"Semantic version\"},\n",
    "        {\"name\": \"created_at\", \"type\": \"date\", \"role\": \"timestamp\", \"description\": \"Creation time\"},\n",
    "        {\"name\": \"announced_at\", \"type\": \"date\", \"role\": \"timestamp\", \"description\": \"Announcement time\"},\n",
    "        {\"name\": \"published_at\", \"type\": \"date\", \"role\": \"timestamp\", \"description\": \"Publication time\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "cicd_ops = {\n",
    "    \"stage_operators\": [\"$match\", \"$group\", \"$sort\", \"$limit\", \"$project\", \"$unwind\", \"$count\"],\n",
    "    \"expression_operators\": [\n",
    "        \"$sum\", \"$avg\", \"$min\", \"$max\", \"$first\", \"$last\",\n",
    "        \"$eq\", \"$ne\", \"$gt\", \"$gte\", \"$lt\", \"$lte\",\n",
    "        \"$in\", \"$nin\", \"$exists\", \"$regex\",\n",
    "        \"$and\", \"$or\",\n",
    "        \"$year\", \"$month\", \"$dayOfMonth\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=== Artifacts Collection ===\\n\")\n",
    "for intent in [\n",
    "    \"Show the latest rhaiis containers\",\n",
    "    \"How many artifacts per product?\",\n",
    "    \"Find all disk-images built for aarch64\",\n",
    "    \"Count artifacts by type\",\n",
    "]:\n",
    "    output = generate_query(artifacts_schema, cicd_ops, intent)\n",
    "    print(f\"Q: {intent}\")\n",
    "    try:\n",
    "        print(f\"A: {json.dumps(json.loads(output), indent=2)}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"A (raw): {output}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Drops Collection ===\\n\")\n",
    "for intent in [\n",
    "    \"Show all rhel-ai drops sorted by creation date\",\n",
    "    \"How many drops per product?\",\n",
    "    \"Find drops announced after January 2025\",\n",
    "]:\n",
    "    output = generate_query(drops_schema, cicd_ops, intent)\n",
    "    print(f\"Q: {intent}\")\n",
    "    try:\n",
    "        print(f\"A: {json.dumps(json.loads(output), indent=2)}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"A (raw): {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Accuracy\n",
    "\n",
    "Run the full eval and held-out sets through the model and measure syntax validity and exact match rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def evaluate_split(split_name: str, max_examples: int = 50):\n",
    "    \"\"\"Evaluate model accuracy on a dataset split.\"\"\"\n",
    "    examples = ds[split_name]\n",
    "    n = min(len(examples), max_examples)\n",
    "\n",
    "    syntax_ok = 0\n",
    "    exact_match = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        ex = examples[i]\n",
    "        start = time()\n",
    "        output = generate_query(ex[\"schema\"], ex[\"allowed_ops\"], ex[\"intent\"])\n",
    "        total_time += time() - start\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(output)\n",
    "            syntax_ok += 1\n",
    "            if parsed == ex[\"output\"]:\n",
    "                exact_match += 1\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  {i+1}/{n} done...\")\n",
    "\n",
    "    avg_latency = total_time / n\n",
    "    print(f\"\\n{split_name} ({n} examples):\")\n",
    "    print(f\"  Syntax valid: {syntax_ok}/{n} ({100*syntax_ok/n:.1f}%)\")\n",
    "    print(f\"  Exact match:  {exact_match}/{n} ({100*exact_match/n:.1f}%)\")\n",
    "    print(f\"  Avg latency:  {avg_latency:.2f}s\")\n",
    "\n",
    "# Evaluate on first 50 examples from each split\n",
    "# (set max_examples higher for full evaluation)\n",
    "evaluate_split(\"eval\", max_examples=50)\n",
    "evaluate_split(\"held_out\", max_examples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "The model was fine-tuned using **QLoRA** — a parameter-efficient method that:\n",
    "\n",
    "1. **Quantizes** the base model to 4-bit (NF4), reducing memory from ~14GB to ~4GB\n",
    "2. **Adds small trainable adapters** (LoRA, rank 8) to attention and MLP layers — only ~0.1% of parameters are trained\n",
    "3. **Trains on synthetic data** — 19 MongoDB schemas × 10 query patterns × augmentation = ~1,300 examples\n",
    "4. **Masks prompt tokens** — the model only learns to generate the JSON query, not to repeat the schema\n",
    "\n",
    "The key insight: each field in the schema has a **semantic role** (`identifier`, `measure`, `timestamp`, `category`, `enum`, `boolean`, `text`) that tells the model how to use it. A `measure` field gets summed/averaged in `$group`, a `timestamp` gets range-filtered with `$gte`/`$lte`, an `enum` gets filtered with `$in`. The model learned to read these roles and compose queries accordingly — even for schemas it has never seen.\n",
    "\n",
    "For more details, see the [GitHub repository](https://github.com/jangel97/text-to-mongo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Baseline Comparison — Before vs After\n",
    "\n",
    "What does the base model produce **without** the LoRA adapter? Let's disable the adapter and run the same examples to see the difference fine-tuning makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the LoRA adapter to get the base model's zero-shot output\n",
    "model.disable_adapter_layers()\n",
    "\n",
    "# Pick examples from both eval and held-out splits\n",
    "comparison_examples = [ds[\"eval\"][0], ds[\"eval\"][5], ds[\"eval\"][10], ds[\"held_out\"][0], ds[\"held_out\"][5]]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE (zero-shot, no adapter)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_matches = 0\n",
    "for i, ex in enumerate(comparison_examples):\n",
    "    output = generate_query(ex[\"schema\"], ex[\"allowed_ops\"], ex[\"intent\"])\n",
    "    print(f\"\\n--- Example {i+1}: {ex['schema']['collection']} ---\")\n",
    "    print(f\"Intent:   {ex['intent']}\")\n",
    "    print(f\"Expected: {json.dumps(ex['output'])}\")\n",
    "    print(f\"Got:      {output}\")\n",
    "    try:\n",
    "        if json.loads(output) == ex[\"output\"]:\n",
    "            baseline_matches += 1\n",
    "            print(\"Match: yes\")\n",
    "        else:\n",
    "            print(\"Match: no\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Match: no (invalid JSON)\")\n",
    "\n",
    "# Re-enable the LoRA adapter\n",
    "model.enable_adapter_layers()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINE-TUNED (with LoRA adapter)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "finetuned_matches = 0\n",
    "for i, ex in enumerate(comparison_examples):\n",
    "    output = generate_query(ex[\"schema\"], ex[\"allowed_ops\"], ex[\"intent\"])\n",
    "    print(f\"\\n--- Example {i+1}: {ex['schema']['collection']} ---\")\n",
    "    print(f\"Intent:   {ex['intent']}\")\n",
    "    print(f\"Expected: {json.dumps(ex['output'])}\")\n",
    "    print(f\"Got:      {output}\")\n",
    "    try:\n",
    "        if json.loads(output) == ex[\"output\"]:\n",
    "            finetuned_matches += 1\n",
    "            print(\"Match: yes\")\n",
    "        else:\n",
    "            print(\"Match: no\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Match: no (invalid JSON)\")\n",
    "\n",
    "n = len(comparison_examples)\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Baseline:   {baseline_matches}/{n} correct ({100*baseline_matches/n:.0f}%)\")\n",
    "print(f\"Fine-tuned: {finetuned_matches}/{n} correct ({100*finetuned_matches/n:.0f}%)\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusion\n\nThe baseline isn't dumb — it's undisciplined. Look at the baseline outputs: the model *understands* MongoDB. It picks the right operators, references the right fields, builds logically correct queries. But it fails on three things the fine-tuning drills in:\n\n1. **Output format** — The baseline wraps everything in `` ```json `` markdown fences instead of raw JSON. That alone makes every single output unparseable by downstream code.\n\n2. **Schema compliance** — It uses `\"query\"` instead of `\"filter\"`, picks operators outside the allowed list (e.g. `$dateToString` instead of `$dayOfMonth`), and invents its own alias names (`min_session_length`, `max_price`) instead of following the training format. The base model draws from its general MongoDB knowledge rather than reading the specific prompt constraints.\n\n3. **Date format** — It uses JavaScript `new Date()` constructors instead of Extended JSON `{\"$date\": \"...\"}`. Correct in a mongo shell, but not parseable as JSON.\n\nThe fine-tuned model produces **byte-identical output** to expected — not just structurally correct, but exact matches. It learned the precise output contract: raw JSON, `filter` not `query`, Extended JSON dates, specific naming conventions.\n\nThe held-out examples (`museum_exhibits`, `weather_stations`, `fleet_vehicles`) prove the model learned to **read schemas** rather than memorize collection-specific patterns.\n\n**The takeaway**: we didn't teach the model MongoDB — it already knew MongoDB. We taught it to follow a strict output protocol, and that's exactly the kind of narrow behavioral constraint where LoRA shines over prompt engineering.",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}